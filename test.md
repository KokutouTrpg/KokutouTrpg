バッチサイズ[100,1000,10000]と最適化[RMSprop,Adam,SGD]を変えて、100エポックまで学習
SGDは10,100で過学習は起きないが、全体として訓練データの学習が及ばなん
RMSとAdamでは100と1000では、君ℜ年が0.99代に達したが、過学習が起きている
10000は過学習は少なかったが、収束が遅く学習完了するまでのエポックがかなりかかかりそう
⇒RMSとAdamの1000で過学習をおさえられないか
dropoutによるパラメータの削減とアンサンブル的学習
一番変数が多くなっているflattenの後にdropout0.5を入れる
⇒数値的には2～3%程度の増加テスト正解率の増加だが、
なしの時では　テストデータに対するロスが加工した後に跳ね上がることがあったが、
アリの時では、下げ幅の差はあるもののテストデータのロスの下降が続いたことから
過学習は抑えられたと考えられ、より学習回数を伸ばすことで値の改善が可能と考えられる。

これとは別にpooling層による情報の捨てを減らすために畳み込みによるプーリングを実装

